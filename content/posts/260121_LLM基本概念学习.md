---
title: "LLMåŸºæœ¬æ¦‚å¿µå­¦ä¹ "
date: 2026-01-21T19:37:43+09:00
description: ""
categories: ["study"]
tags: ["LLM"]
draft: false
---

## èƒŒæ™¯ / ç›®çš„ï¼ˆWhyï¼‰

- æ„è¯†åˆ°æˆ‘å¯¹computer scienceæ¦‚å¿µçš„è®¤çŸ¥è¿˜åœç•™åœ¨2020å¹´æ¯•ä¸šçš„æ—¶å€™å­¦ä¹ çš„æ¦‚å¿µã€‚
- æˆ‘äº†è§£machine learningçš„ç®—æ³•ã€CNNçš„ç»“æ„ã€NLPçš„åŸºç¡€æ“ä½œï¼Œä½†è¿™äº›å†…å®¹å·²ç»out of date
- æ›´æ–°csç›¸å…³çŸ¥è¯†ï¼Œç†è§£llmçš„ç®—æ³•æœºåˆ¶

<!--more-->
## å†…å®¹ï¼ˆContentï¼‰

### LLMæ˜¯ä»€ä¹ˆ
åŸºäºTransformeræ¶æ„çš„å¤§ç¥ç»ç½‘ç»œ 

*Transformeræ¶æ„çš„æœºåˆ¶ï¼š* 

*1. Self Attention*ï¼šæ¯ä¸ªtokenè§‚å¯Ÿå…¨ä½“token  
- å¯¹äºæ¯ä¸ªtokenï¼Œè®¡ç®—å®ƒå¯¹æ‰€æœ‰tokençš„ç›¸å…³æƒé‡ï¼Œå†å°†æ‰€æœ‰tokenä¿¡æ¯æŒ‰æƒé‡ç›¸åŠ ï¼Œå¾—åˆ°æ–°è¡¨ç¤ºã€‚attentionè¾“å‡ºçš„æ˜¯valueçš„åŠ æƒå’Œã€‚
- ä»è¾“å…¥åºåˆ—Xä¸­ç”ŸæˆQ, K, V å‘é‡:
    - Queryï¼šå½“å‰tokenæƒ³è¦å¯»æ‰¾ä»€ä¹ˆä¿¡æ¯
    - Keyï¼šå½“å‰tokenèƒ½æä¾›ä»€ä¹ˆç‰¹å¾ä¾›æ£€ç´¢
    - Valueï¼šå½“å‰tokenåŒ…å«çš„å…·ä½“è¯­ä¹‰å†…å®¹

*2. Scaled Dot-Product Attention*: æ€ä¹ˆè®¡ç®—tokenä¹‹é—´çš„ç›¸å…³æ€§aij  
- å¾—åˆ° Q, K, V å‘é‡
- ç‚¹ç§¯è®¡ç®—ç›¸ä¼¼åº¦
    - Scalingï¼šç‚¹ç§¯ç»“æœé™¤ä»¥æ ¹å·ä¸‹å‘é‡ç»´åº¦dkæ¥ç¨³å®šå°ºåº¦ï¼Œé¿å…ç‚¹ç§¯è¿‡å¤§,softmaxè¿‡å°–ï¼Œå¯¼è‡´æ¢¯åº¦é€€åŒ–ï¼ˆç¨€ç–/ä¸ç¨³å®š/å­¦ä¸åŠ¨ï¼‰
- softmaxæ˜ å°„æˆæ¦‚ç‡æƒé‡
- å¯¹ValueåŠ æƒæ±‚å’Œ

*3. Multi-head Attention*ï¼šä¸Šè¿°è¿‡ç¨‹å¹¶è¡Œè¿è¡Œå¤šæ¬¡
- å…è®¸ä¸åŒç»´åº¦åŒæ—¶ç†è§£è¯­è¨€ï¼ˆå°†åŒä¸€è¾“å…¥é€šè¿‡ä¸åŒçš„çº¿æ€§æŠ•å½±çŸ©é˜µæ˜ å°„åˆ°å¤šä¸ªå­ç©ºé—´ï¼‰ï¼Œå­¦ä¹ ä¸åŒå…³ç³»æ¨¡å¼
- æœ€åæ•´åˆæ‰€æœ‰å¤´çš„è¾“å‡ºï¼š
    - Concatenationï¼šæŠŠæ‰€æœ‰attention headçš„è¾“å‡ºå‘é‡æ‹¼æˆä¸€ä¸ªé•¿å‘é‡
    - Output Projectionï¼šé•¿å‘é‡ä¹˜ä»¥ä¸€ä¸ªåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ çš„æƒé‡çŸ©é˜µWo
        - Woé€šè¿‡åå‘ä¼ æ’­ï¼Œä»lossçš„æ¢¯åº¦ï¼ˆä¾‹å¦‚ next-token lossï¼‰ä¸­å­¦ä¹ /æ›´æ–°å‚æ•°
        - Woå­¦ä¹ äº†å“ªäº›headè¾“å‡ºå¯¹é™ä½lossæœ‰å¸®åŠ©ï¼Œå¦‚ä½•æŠŠæ‹¼æ¥åä¿¡æ¯ä»¥æœ€æœ‰ç”¨çš„å½¢å¼æ¢å¤ç»´åº¦  

*4. Residual Connection*ï¼š æŠŠå±‚çš„è¾“å…¥ç›´æ¥åŠ åˆ°è¯¥å±‚è¾“å‡ºä¸Š
- ç¼“è§£åå‘ä¼ æ’­æ—¶æ¢¯åº¦æ¶ˆå¤±
- åŠ å±‚ä¸ç”¨æ¯ä¸€å±‚é‡å­¦å®Œæ•´æ˜ å°„æ„å»ºæ–°è¡¨ç¤ºï¼Œåªå­¦å¢é‡ä¿®æ­£
    - MHAçš„å¢é‡ï¼šè·¨tokençš„ä¿¡æ¯
    - FFNçš„å¢é‡ï¼šé€tokenæå–çš„æ–°ç‰¹å¾ç»„åˆ
- æ·±å±‚ç½‘ç»œä¸ä¼šå› å±‚æ•°å¢åŠ è¢«è¿«è¯¯å·®ä¸Šå‡ï¼ˆå¯æ¨¡æ‹Ÿæµ…å±‚è§£ï¼‰
- æ®‹å·®é“¾æ¥è¦æ±‚è¾“å…¥ä¸è¾“å‡ºç»´åº¦ä¸€è‡´ï¼Œè¿«ä½¿ç»“æ„è®¾è®¡ä¸Šå›ºå®šæ¯å±‚çš„è¡¨ç¤ºç»´åº¦ï¼Œè¯¥ç»“æ„ä¾¿äºå±‚æ•°å †å 

*5. Layer Normalization*ï¼šå¯¹tokençš„ç‰¹å¾ç»´åº¦ embedding dim åšå½’ä¸€åŒ–
- æ ¡å‡†åˆ°ç¨³å®šæ•°å€¼åŒºé—´ï¼Œæ§åˆ¶å±‚æ•°å åŠ å¯¼è‡´çš„å°ºåº¦æ¼‚ç§»
    - å‡å€¼æ‹‰åˆ°0ï¼Œæ–¹å·®æ‹‰åˆ°1ï¼Œå†ç”¨å‚æ•°è°ƒå›åˆé€‚å°ºåº¦
    - ğ›¾ ï¼šå¯å­¦ä¹ ç¼©æ”¾ï¼Œå­¦â€œéœ€è¦å¤šå¤§å¹…åº¦â€
    - Î²ï¼šå¯å­¦ä¹ å¹³ç§»ï¼Œå­¦â€œéœ€è¦ä»€ä¹ˆåŸºçº¿åç½®â€
- é€šè¿‡æ§åˆ¶xå°ºåº¦é˜²æ­¢æ³¨æ„åŠ›è®¡ç®—ï¼ˆ2ï¼‰ä¸­çš„softmaxæ¢¯åº¦æ¶ˆå¤±
- é€šè¿‡æ§åˆ¶xå°ºåº¦å‡å°‘æ·±å±‚ç½‘ç»œé“¾å¼æ³•åˆ™è¿ä¹˜å¯¼è‡´çš„æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸æ¦‚ç‡

*6. Feed-Forward Network*ï¼šå¯¹æ¯ä¸ªtokençš„å‘é‡è¿›è¡Œéçº¿æ€§å˜åŒ–ï¼Œæå–æ›´å¤æ‚çš„è¯­ä¹‰ç‰¹å¾
- ç”±ä¸¤å±‚Multi-layer Perceptronæ„æˆï¼ŒFC layer+éçº¿æ€§æ¿€æ´»
- äº§ç”Ÿä¸€ä¸ªä¿®æ­£å‘é‡ï¼Œä½œä¸ºæ®‹å·®ä¸è¾“å…¥ç›¸åŠ 

*7. Positional Encoding*ï¼šä¸ºtokenæ·»åŠ é¡ºåºä¿¡æ¯ï¼Œè¯†åˆ«ç»å¯¹å’Œç›¸å¯¹ä½ç½®
- ç»å¯¹ä½ç½®ç¼–ç 
- ç›¸å¯¹ä½ç½®ç¼–ç 
- æ—‹è½¬ä½ç½®ç¼–ç  RoPE

*Transfomerçš„ä¸€æ¬¡å‰å‘ä¼ æ’­æµç¨‹*

*è¾“å…¥é˜¶æ®µ*ï¼šX
- tokenization + embedding + positional encoding

*è¿›å…¥Transformer Block*ï¼š
- LayerNorm ï¼š U = LN(X)
- MHA: ä¸Šè¿°æåˆ°ï¼ˆ1ï¼‰ï¼ˆ2ï¼‰ï¼ˆ3ï¼‰
- å¤šä¸ªattention head
- æ¯ä¸ªheadå†…éƒ¨ï¼š
    - çº¿æ€§æŠ•å½±å¾—Q, K, V
    - è®¡ç®—tokenå¯¹æ‰€æœ‰tokençš„ç›¸å…³æ€§æƒé‡ï¼ˆå«scalingï¼‰
    - softmaxæ˜ å°„æˆæ¦‚ç‡æƒé‡ï¼Œè®¡ç®—Vçš„åŠ æƒå’Œ
    - multiheadè¾“å‡º -> concat -> output projection
- Residual: X <- X + MHA(U) åŠ ä¸Šæ³¨æ„åŠ›å¢é‡
- LayerNorm: U'= LN(X)
- FFNï¼šå¯¹ç‰¹å¾ç»´åº¦éçº¿æ€§åŠ å·¥
- Residualï¼š X <- X + FFN(U')

*åˆ°æœ€åä¸€å±‚*ï¼šè¯­è¨€æ¨¡å‹å¤´åšé¢„æµ‹



